---
title: "Addressing Subposterior Conflict With Weighted Proposals"
author: "Andrew Manderson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: tgpagella
fontsize: 10pt
papersize: a4
geometry: margin=2.25cm
bibliography: ../0bibliography/year-1-bib.bib
csl: aam71-test.csl
output: 
  pdf_document:
    includes:
      in_header:
        tex-input/pre.tex
    fig_caption: true
    number_sections: true
    keep_tex: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, out.width = "100%", fig.align = "center", auto_pdf = TRUE, cache = TRUE, fig.width = 5, fig.asp = 0.5)
sim_pars <- readRDS("rds/norm-norm-ex/sim-pars.rds")
attach(sim_pars) # Needed to dynamically generate table
```

The need for accurate self-density ratio estimates arose from a conflict between $(m - 1)^{\text{th}}$ stage posterior and the unknown $m^{\text{th}}$ model's prior marginal distribution. 
This was purely a disagreement of scale, the distributions in question did not have disjoint supports.
This document considers subposteriors with disjoint supports, and demonstrates some potential solutions.

# Disjoint subposteriors

Consider the two subposteriors $\pd_{1}(\phi \mid Y_{1})$, $\pd_{2}(\phi \mid Y_{2})$, and melded posterior $\pmeld(\phi \mid Y_{1}, Y_{2})$ shown in Figure&nbsp;\ref{fig:subpost_disagreement}.
Using the samples of either of the subposteriors as a proposal for the overall posterior results in a degenerate sample of the overall posterior, due to an insufficient number of samples in the support of $\pmeld(\phi \mid, Y_{1}, Y_{2})$.
If instead we sample a version of one of the subposteriors which is _augmented_ by a function $\tarw_{1}(\phi; \eta_{1})$, i.e.&nbsp;$\pd_{1}(\phi \mid Y_{1})\tarw_{1}(\phi; \eta_{1})$, we can control the stage two acceptance probability through $\tarw_{1}(\phi; \eta_{1})$.
The green line in Figure&nbsp;\ref{fig:subpost_disagreement} illustrates how we can suitably scale and shift the stage one target. 

```{r subpost_disagreement, fig.cap = "Hypothetical conflicting subposteriors (blue), melded posterior (red), and the augmented stage one one target (green, dashed)."}
knitr::include_graphics("plots/intro/subpost-disagreement.pdf")
```

The general problem concerns instances where the support of $\pd_{\text{meld}, \modelindex - 1}(\phi, \psi_{1}, \ldots, \psi_{\modelindex - 1} \mid Y_{1}, \ldots, Y_{\modelindex - 1})$ does not intersect with the support of $\pd_{\text{meld}, \modelindex}(\phi, \psi_{1}, \ldots, \psi_{\modelindex} \mid Y_{1}, \ldots, Y_{\modelindex})$, hence also does not overlap with $\pd_{\modelindex}(\phi,~\psi_{\modelindex}~\mid~Y_{\modelindex})$.
We must choose the functional form of $\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$, as well as the parameter $\eta_{\modelindex - 1}$, such that the supports of $\pd_{\text{meld}, \modelindex - 1}(\phi, \psi_{1}, \ldots, \psi_{\modelindex - 1} \mid Y_{1}, \ldots, Y_{\modelindex - 1})\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$ and $\pd_{\text{meld}, \modelindex}(\phi, \psi_{1}, \ldots, \psi_{\modelindex} \mid Y_{1}, \ldots, Y_{\modelindex})$ intersect.

# Methods - Maths

Consider the overall melded target posterior for $\Nm = 2$ models
\input{tex-input/method/0010-melded-target-posterior.tex}
The _augmented stage one target_ is then
\input{tex-input/method/0020-stage-one-target.tex}
Hence, the stage two acceptance probability is 
\input{tex-input/method/0030-stage-two-acceptance.tex}

The presence of the $\tarw_{1}(\phi; \eta_{1})$ terms in the stage 2 acceptance probability implies that we have some degree of control over the acceptance rate.
The $\pd_{2}(\phi) \tarw_{1}(\phi; \eta_{1})$ term also suggests that this idea may be more linked than we suspect to our weighted estimation methodology for the prior marginal distribution.

For $\Nm > 2$, we will accrue an additional $\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$ term for each additional model, which may be problematic.

## Choosing $\tarw_{1}(\phi; \eta_{1})$

Say we use the samples of $\phi$ from 
\input{tex-input/method/0009-intermediary-meld.tex}
as a proposal for $\pd_{\text{meld}}(\phi, \psi_{1}, \psi_{2} \mid Y_{1}, Y_{2})$ and observe the behaviour in Figure&nbsp;\ref{fig:no_u_traces}.
We can still use these samples, in combination with samples from $\pd_{2}(\phi, \psi_{2} \mid Y_{2})$, to choose an appropriate $\tarw_{1}(\phi; \eta_{1})$.

Consider a sample of size $\Nx$ from the intermediary melded target $\pd_{\text{meld}, 1}(\phi, \psi_{1} \mid Y_{1})$ denoted $\{\tilde{\phi}_{\sampleindex, 1}\}_{\sampleindex = 1}^{\Nx}$.
Using this sample we compute the sample mean of the first stage target denoted $\tilde{\mu}_{1} = \frac{1}{\Nx}\sum\limits_{\sampleindex = 1}^{\Nx}\tilde{\phi}_{\sampleindex, 1}$, as well as the sample variance $\tilde{\sigma}^{2}_{1} = \frac{1}{\Nx - 1}\sum\limits_{\sampleindex = 1}^{\Nx} (\tilde{\phi}_{\sampleindex, 1} - \tilde{\mu}_{1})^{2}$.
Analogous quantities for the second subposterior can then be defined using the sample of size $\Nx$ from the second subposterior[^wrong] $\pd_{2}(\phi, \psi_{2} \mid Y_{2})$ as $\{\phi_{\sampleindex, 2}\}_{\sampleindex = 1}^{\Nx}$.
The sample mean and variance of the subposterior, $\hat{\mu}_{2}$ and $\hat{\sigma}^{2}_{2}$, are computed in the same manner.

[^wrong]: This should probably be the stage two target, not the subposterior. Again, ignoring the impact of the prior?

These sample statistics can then be used to define an appropriate augmentation function $\tarw_{1}(\phi; \eta_{1})$.
We now have to assume that the intermediary target and subposterior distributions can be appropriately summarised by Gaussian distributions.
Denoting the Gaussian density function with mean $\mu$ and variance $\sigma^2$ as $f(\phi; \mu, \sigma^2)$, we define the augmentation function as a ratio of Gaussian densities
\input{tex-input/method/0040-augmentation-definition.tex}
whose parameters we will define momentarily.
The intuition is that the denominator density should approximate the intermediary melded target $\pd_{\text{meld}, 1}(\phi, \psi_{1} \mid Y_{1})$, whilst the numerator should approximate $\pd_{\text{meld}, 2}(\phi, \psi_{1}, \psi_{2} \mid Y_{1}, Y_{2})$, which is a combination of both the first stage target and the second subposterior.

Using this intuition we define the parameters of the denominator density function to be $\mu_{\text{de}} = \tilde{\mu}_{1}$ and $\sigma^{2}_{\text{de}} = \tilde{\sigma}^{2}_{1}$. 
We can use the well known results about products of Gaussian density functions, summarised in @bromiley:03, to compute the numerator parameters
\input{tex-input/method/0050-numerator-results.tex}
and for compactness we define $\eta_{1} = \{\mu_{\text{nu}}, \sigma^{2}_{\text{nu}}, \mu_{\text{de}}, \sigma^{2}_{\text{de}}\}$.

In cases where all densities of interest are Gaussian, $f(\phi; \mu_{\text{de}}, \sigma^2_{\text{de}})$ precisely summarises the typical stage one melding target $\pd_{1}(\phi, \psi_{1}, Y_{1}) \mathop{/} \pd_{1}(\phi)$. 
This results in an augmented target proportional to $f(\phi; \mu_{\text{nu}}, \sigma^2_{\text{nu}})$, which we have defined as a composition of the two distributions of interest.

If $\pd_{1}(\phi, \psi_{1}, Y_{1}) \mathop{/} \pd_{1}(\phi)$ has heavier tails than a Gaussian distribution, we may have concerns about the integrability of the target in Equation&nbsp;\eqref{eqn:stage-one-target}.
However, by construction $\sigma_{\text{nu}}^{2} < \sigma_{\text{de}}^{2}$, which ensures that $\tarw_{1}(\phi; \eta_{1})$ is integrable.
Given the original target $\pd_{1}(\phi, \psi_{1}, Y_{1}) \mathop{/} \pd_{1}(\phi)$ is integrable[^really], and the augmented target is the product of two integrable densities, the augmented target is also integrable.
This gives the practitioner some room to tweak $\tarw_{1}(\phi; \eta_{1})$; as long as $\sigma_{\text{nu}}^{2} < \sigma_{\text{de}}^{2}$ is satisfied, the target is integrable.
Thus $\sigma_{\text{nu}}^{2}$ can be artificially increased, to a known limit, to partially ameliorate the impact of the approximation error. 

[^really]: Not obviously true without pooled prior term.

## $\Nm > 2$ 

Applying this methodology in the $\Nm > 2$ case is awkward.
Disagreement between stage $\modelindex - 1$ and $\modelindex$ requires sampling an augmented version of $\modelindex - 1$.
When using the multi-stage MH-within-Gibbs setting, this requires resampling all previous $\modelindex - 1$ stages.
Each of these stages will require augmenting, in a stage specific way, to ensure that the interstage conflict is not simply 'moved' to earlier stages.

Notationally, the interstage conflict would exist between the intermediary melded target $\pd_{\text{meld}, \modelindex - 1}(\phi, \psi_{1}, \ldots, \psi_{\modelindex - 1}\mid Y_{1}, \ldots, Y_{\modelindex - 1})$ and the $m^{\text{th}}$ subposterior $\pd_{\modelindex}(\phi, \psi_{\modelindex} \mid Y_{\modelindex})$.
The augmentation function for the $(\modelindex - 1)^{\text{th}}$ stage target is $\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$.

## Similarities to @vehtari:etal:14

Expectation propagation (EP) is interested in approximating a target density $f(\phi)$, typically a posterior distribution, with a density $g(\phi; \eta)$ from a known parametric family, with parameter $\eta$.
Both densities are assumed to factorise multiplicatively
\input{tex-input/expectation-propagation/0010-factorisations-ep.tex}
It is assumed that each component of the factorisation of $f(\phi)$ has an equivalent component in $g(\phi; \eta)$.
Each component of the factorisations is referred to as a _site_, and the overall approximation $g(\phi; \eta)$ is the _global_ approximation.
@vehtari:etal:14 then define the _cavity distribution_ $g_{-\modelindex}(\phi; \eta)$ and the _tilted distribution_ $g_{\setminus\modelindex}(\phi; \eta)$
\input{tex-input/expectation-propagation/0020-ep-distribution-defs.tex}

Assume we have some initial value for $\eta$ in $g(\phi; \eta)$, which is an initial approximation to the target density.
For each site $\modelindex$, EP proceeds to

1. Compute the cavity distribution $g_{-\modelindex}(\phi; \eta)$, which is an algebraic calculation for each site as $g$ is a member of an exponential family.
1. Improve the site approximation $g_{\modelindex}(\phi; \eta_{\modelindex})$ by optimising $\eta_{\modelindex}$ with respect to a divergence metric, typically the Kullback-Leibler (KL) divergence between $f_{\modelindex}(\phi) g_{-\modelindex}(\phi; \eta)$ and $g_{\modelindex}(\phi; \eta_{\modelindex}) g_{-\modelindex}(\phi; \eta)$.

These two steps can be performed in batch-parallel, where each $\eta_{\modelindex}$ is optimised in parallel then communicated back to a central node.
The central node computes $\eta$, then broadcasts the result back to each parallel node for more optimisation steps, which is repeated until some convergence criteria is reached. @vehtari:etal:14 state that optimising $\eta_{\modelindex}$ is the critical operation in EP, and that minimising the KL divergence corresponds to matching the moments of $f_{\modelindex}(\phi) g_{-\modelindex}(\phi; \eta)$ and $g_{\modelindex}(\phi; \eta_{\modelindex}) g_{-\modelindex}(\phi; \eta)$.

### Analogies in Markov melding

Consider the melded posterior
\input{tex-input/expectation-propagation/0030-melding-factorisation.tex}
noting that the product terms now start at $\modelindex = 1$.
Our definition of $\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$ is  analogous to the cavity distribution $g_{-\modelindex}(\phi; \eta)$ of @vehtari:etal:14.
The denominator density $f(\phi; \mu_{\text{de}}, \sigma^{2}_{\text{de}})$ in Equation&nbsp;\eqref{eqn:augmentation-definition} is an approximation to $\pd_{\modelindex}(\psi_{\modelindex} \mid Y_{\modelindex}, \phi)$ in precisely the way EP suggests approximating[^notation] $f_{\modelindex}(\phi)$ with $g_{\modelindex}(\phi; \eta_{\modelindex})$.
Likewise, the numerator density in Equation&nbsp;\eqref{eqn:augmentation-definition} is acting as the equivalent _global_ approximation $g(\phi; \eta)$ in EP.
There is a slight difference due to the pooled prior, which we do not[^pooledprior] factor into the computation of $\tarw_{\modelindex - 1}(\phi; \eta_{\modelindex - 1})$.

[^notation]: This notation clashes, I should fix that.
[^pooledprior]: Suggesting I'm not doing something right - would be worth factoring in?. I guess I've just been ignoring the impact of the pooled prior.

This suggests adopting the EP strategy to address conflict when melding, which may be particularly useful in the $\Nm > 2$ case.

1. Sample $\ppoolphi$ and the marginalised subposteriors $\pd_{1}(\phi, \psi_{1}, Y_{1}) \mathop{/} \pd_{1}(\phi), \,\, \ldots\,\, , \pd_{\Nm}(\phi, \psi_{\Nm}, Y_{\Nm}) \mathop{/} \pd_{\Nm}(\phi)$ in parallel (on separate nodes).
1. Approximate each distribution with $g_{\modelindex}(\phi; \eta_{\modelindex})$.
1. Send the results back to a central node, and compute the global approximation $g(\phi; \eta)$.
1. Broadcast the global approximation back to each node.
1. Each node then computes their specific cavity distribution, and samples from the resulting tilted distribution.
1. Iterate step 3 -- 5 until there is convergence in the global approximation. 

<!-- - Each (intermediary target / conditional) (and the pooled prior) has its own $\tarw_{\modelindex}(\phi; \eta_{\modelindex})$ that is initially proportional to a constant.
- After a certain number of parallel MCMC samples of the resulting target distributions, each $\tarw_{\modelindex}(\phi; \eta_{\modelindex})$ is updated using the information from the samples of all the target distributions.
- Iterate until there is agreement (divergence metric based on samples? KS test?)
    - KS probably way too sensitive, think of others.
    - What was the combination hypotheses idea Rajen was talking about? Useful here?
    - Convergence properties almost certainly unprovable -->

## Potential issues

- Feasible in 1/2/(3?) dimensions, and melding cases where $\Nm$ is $<10$?. 
    - Trying to think of cases where it is easyish to figure out where the overall posterior will be by visual inspection.
    - More general idea for finding "midpoint" between samples from two distributions.
        - Seems a lot like the idea of Barycentres in: [@srivastava:etal:15; @srivastava:li:dunson:18].
        - Infact, I think this is identical in the 1-D case.
        - In the 2-or-more-D case, we have to start thinking about covariance as well. 
- Can use ESS as a diagnostic for particle degeneracy?
- In the case of no overlap, we are sensitive to the tails of the approximation (in the EP setting).
 
# Example: two Gaussians

Starting from the same Gaussian-Gaussian model used to demonstrate the importance of accurately estimating the prior marginal distribution, we adjust some of the hyperparmeters to manufacture disjoint subposteriors.
There are $\Nm = 2$ models, with common parameter $\phi$,
\input{tex-input/norm-norm-ex/0010-norm-models.tex}
We simulate `r n_y_1` observations from model 1, i.e.&nbsp;$Y_{1}$ is a vector of length `r n_y_1`, and `r n_y_2` observations from model 2.
The observations variances $\varepsilon_{1}^{2}$ and $\varepsilon_{2}^{2}$ are fixed to
`r sigma_1` and `r sigma_2` respectively.
The prior means $\mu_{1}$ and $\mu_{2}$ are `r mu_phi_1` and `r mu_phi_2`, whilst the prior variances are `r sigma_phi_1` and `r sigma_phi_2`.
This configuration results in the subposterior distributions in Figure&nbsp;\ref{fig:subposteriors}.

```{r subposteriors, fig.cap = "Disjoint subposteriors for the two Gaussian example."}
knitr::include_graphics("plots/norm-norm-ex/subposteriors.pdf")
```

The pooled prior is formed by linear pooling. 
For the mulit-stage samplers, the stage one target is defined in Equation&nbsp;\eqref{eqn:stage-one-target}.

## Stage one target and $\text{u}_{1}(\phi)$.

Here we choose $\text{u}_{1}(\phi)$ via the method described in Section&nbsp;\ref{choosing-tarw_1phi-eta_1}. 
Figure&nbsp;\ref{fig:u_func_augmented_target} displays $\text{u}_{1}(\phi)$ (right panel) and the augmented target $\tilde{\pd}_{\text{meld}, 1}(\phi, \psi_{1}, Y_{1})$ (left panel).
The augmented target now lies inbetween the two subposteriors depicted in Figure&nbsp;\ref{fig:subposteriors}.

```{r u_func_augmented_target, fig.cap ="The augmented stage one target (left panel) and corresponding $\\text{u}_{1}(\\phi)$ (right panel)."}
knitr::include_graphics("plots/norm-norm-ex/u-function-augmented-target.pdf")
```

## Numerical tests

We want to compare the melded posterior distribution sampled using the following methods:

- Multi-stage sampler, no augmentation function (standard melding).
- Multi-stage sampler, _with_ augmentation function.
- Directly sampling the melded posterior, as a point of reference.

We can analytically compute the melded posterior, however evaluating is numerically challenging.
I have yet to find the calculations that introduce the numerical inaccuracy I observe. 

#### Sequential MH, with no $\text{u}_{1}(\phi)$ (standard melding)

Here we apply the multi-stage sampler with no augmentation function, or equivalently set $\tarw_{1}(\phi; \eta_{1}) = 1$.
The traceplots in Figure&nbsp;\ref{fig:no_u_traces} demonstrate the issue with this approach, specifically that the support of the first stage does not intersect with the support of the melded posterior.

```{r no_u_traces, fig.cap = "Traceplots for the first and second stage target distributions, with no subposterior augmentation."}
knitr::include_graphics("plots/norm-norm-ex/no-u/stage-traces.pdf")
```

#### Sequential MH, _with_ $\text{u}_{1}(\phi)$ (idea developed here)

Now we sample the augmented target in the first stage, for use in the second and final stage.
Figure&nbsp;\ref{fig:with_u_traces_2} shows a clear improvement in performance over Figure&nbsp;\ref{fig:no_u_traces}. 

```{r with_u_traces_2, fig.cap = "Traceplots for the fist and second stage targets distributions, \\textit{with} augmented stage one target."}
knitr::include_graphics("plots/norm-norm-ex/with-u-2/stage-traces.pdf")
```

####  Joint MH - sampling $\pmeld(\phi \mid Y_{1}, Y_{2})$ directly 

In this example we can specify the melded posterior in Stan directly.
Figure&nbsp;\ref{fig:joint_trace} is the traceplot of the sampler that directly targets the melded posterior.

```{r joint_trace, fig.cap = "Traceplot of the directly sampled melded posterior."}
knitr::include_graphics("plots/norm-norm-ex/joint/trace.pdf")
```

We compare the sample quantiles of the melded joint posterior and the stage two melded posterior obtained using the augmented stage one target in Figure&nbsp;\ref{fig:joint_augmented_compare}, and we see good agreement between the two samples.

```{r joint_augmented_compare, fig.cap = "Quantile-quantile plot of sample quantiles obtained by directly sampling the melded joint posterior (y-axis) and the stage two sample quantiles obtained using the augmented stage one target (x-axis)."}
knitr::include_graphics("plots/norm-norm-ex/joint-augmented-compare.pdf")
```

# Computational costs

## Multi-stage Metropolis-Hastings within Gibbs

### Two submodel case {-}


We would like to generate $\Nx$ samples from the overall melded posterior, and hence opt to generate $\Nx$ samples from the stage one target.
Each stage one sample has a computational cost of order $\mathcal{O}(C_{1})$ which involves evaluating the unnormalised logposterior ($\mathcal{O}(C_{1, \text{lp}})$), and generating and evaluating a proposal for the link parameter ($\mathcal{O}(C_{1, \mathcal{Q}})$).
Note that $\mathcal{O}(C_{1}) = \mathcal{O}(C_{1, \text{lp}} + C_{1, \mathcal{Q}})$.
The equivalent set of costs exist for the second stage target.

First we have to identify that we have a problem:

- The cost of sampling the original stage one target: $\mathcal{O}(\Nx C_{1})$.
- The cost of evaluating said samples in stage two: $\mathcal{O}(\Nx C_{2, \text{lp}})$.

Now we can solve it by:

- Sampling the second stage subposterior: $\mathcal{O}(\Nx C_{2})$.
    - This does not include the pooled prior.
- Sampling the augmented stage one target: $\mathcal{O}(\Nx C_{1}')$.
    - The prime indicates that the augmented target has strictly greater computational cost to evaluate.
- Evaluating the augmented samples in stage two: $\mathcal{O}(\Nx C_{2, \text{lp}})$.

### More than two submodels {-}

Fri 26 Jul 13:16:49 2019: _I don't think this is correct, consider three subposteriors in a triangle._

If we have $\Nm > 2$ models, then the costs grow in the following way.

Identifying the problem:

- We finish sampling the $\modelindex-1$th stage target: $\mathcal{O}(\Nx (C_{1}  + C_{2, \text{lp}} + \ldots + C_{\modelindex - 1, \text{lp}}))$.
- Then, we evaluate these samples under the $\modelindex$th stage: $\mathcal{O}(\Nx C_{\modelindex, \text{lp}})$ and realise we have conflict.

Addressing the issue:

- Sampling all the subposteriors: $\mathcal{O}(\Nx (C_{1} + \ldots + C_{\Nm}))$
- Sampling the augmented stage one target $\mathcal{O}(\Nx C_{1}')$.
- Evaluating them under the augmented intermediary targets: $\mathcal{O}(\Nx (C_{2, \text{lp}}' + \ldots + C_{\Nm, \text{lp}}))$.
    - Unless we augmented all the intermediary targets, any one stage could render the sample degenerate.
    - We do __not__ augment the final target.


## Sequential Monte Carlo (SMC) 

Adopting an SMC approach alleviates the need for initial runs to both diagnose conflict, and calculate an appropriate augmentation function $\tarw_{1}(\phi; \eta_{1})$.
Consider the $\Nm = 2$ models case initially.
If introducing each submodel requires $T$ tempering steps, each with a single refreshment step, and we wish to generate $\Nx$ samples, the computational costs include:

- Sampling the stage one target: $\mathcal{O}(\Nx C_{1})$.
- Evaluating the $T$ tempered likelihoods, and then refreshing the particles: $\mathcal{O}(\Nx T (C_{1} + C_{2}))$.
    <!-- - Tempered likelihood strictly has a different cost, and the MCMC refreshment step needs to target something slightly augmented. -->
- The overall cost is then $\mathcal{O}(\Nx T (2 C_{1} + C_{2}))$.

We can extrapolate this to $\Nm > 2$ models.
Assume introducing each additional submodel also requires $T$ tempering steps.
The final computational cost is then $\mathcal{O}(\Nx T (\Nm C_{1} + (\Nm - 1) C_{2} + \ldots + C_{\Nm}))$.
Perhaps there are ways to reduce the number of times we need to evaluate the previous models when introducing the $\modelindex$th model.
A careful reading of [@lindsten:etal:17] seems prudent, as the naive SMC algorithm I am envisaging here involves evaluating the joint model (i.e. all the data).


# Example - real world? Where can we get disjoint conflict from.

- Investigate the Carlin example Rob sent

<!-- -------------------- END OF MAIN BODY OF DOCUMENT -------------------- -->
\newpage

<!-- The {-} tag here suppresses the section numbering. -->
# Bibliography {-}

<!-- This makes pandoc-citeproc put the references before the end of document. -->
<div id="refs"></div>

\newpage

<!-- Now switch to alphabetical numbering for the appendix, and reset the counter. -->
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}

# Appendix 